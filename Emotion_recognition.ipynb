{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emotion_recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/binod132/DATAMINNING/blob/master/Emotion_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq56fthlZ_7a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import moviepy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTYwLL1JapMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import menpo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOX_XoICas21",
        "colab_type": "code",
        "outputId": "5606689c-cf92-4e8e-8fb4-765a5db17cb6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: menpo in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.6/dist-packages (from menpo) (1.3.0)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from menpo) (4.3.0)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from menpo) (1.16.4)\n",
            "Requirement already satisfied: matplotlib>=3.0 in /usr/local/lib/python3.6/dist-packages (from menpo) (3.0.3)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.0->menpo) (0.46)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->menpo) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->menpo) (2.4.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->menpo) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0->menpo) (2.5.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib>=3.0->menpo) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0->menpo) (41.0.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8FLn_vfa2cc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import menpo"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOWoaZ_dbRtt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1-Immjibf2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from io import BytesIO\n",
        "from pathlib import Path\n",
        "from moviepy.editor import VideoFileClip\n",
        "from menpo.visualize import progress_bar_str, print_progress\n",
        "from moviepy.audio.AudioClip import AudioArrayClip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOToavgebkRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "root_dir = Path('path_of_RECOLA')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCrFPWM_bycO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "portion_to_id = dict(\n",
        "    train = [], \n",
        "    valid = [],\n",
        "    test  = [] \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MQiXJUkob59_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_samples(subject_id):\n",
        "    arousal_label_path = root_dir / 'Ratings_affective_behaviour_CCC_centred/arousal/{}.csv'.format(subject_id)\n",
        "    valence_label_path = root_dir / 'Ratings_affective_behaviour_CCC_centred/valence/{}.csv'.format(subject_id)\n",
        "\n",
        "    clip = VideoFileClip(str(root_dir / \"Video_recordings_MP4/{}.mp4\".format(subject_id)))\n",
        "\n",
        "    subsampled_audio = clip.audio.set_fps(16000)\n",
        "\n",
        "    audio_frames = []\n",
        "    for i in range(1, 7501):\n",
        "        time = 0.04 * i\n",
        "\n",
        "        audio = np.array(list(subsampled_audio.subclip(time - 0.04, time).iter_frames()))\n",
        "        audio = audio.mean(1)[:640]\n",
        "\n",
        "        audio_frames.append(audio.astype(np.float32))\n",
        "\n",
        "    arousal = np.loadtxt(str(arousal_label_path), delimiter=',')[:, 1][1:]\n",
        "    valence = np.loadtxt(str(valence_label_path), delimiter=',')[:, 1][1:]\n",
        "\n",
        "    return audio_frames, np.dstack([arousal, valence])[0].astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOOmsMYXb_06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_jpg_string(im):\n",
        "    # Gets the serialized jpg from a menpo `Image`.\n",
        "    fp = BytesIO()\n",
        "    menpo.io.export_image(im, fp, extension='jpg')\n",
        "    fp.seek(0)\n",
        "    return fp.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkwB132ScPS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _int_feauture(value):\n",
        "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Vke9NXBcUwq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _bytes_feauture(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLohjLgFcVVm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def serialize_sample(writer, subject_id):\n",
        "    subject_name = 'P{}'.format(subject_id)\n",
        "\n",
        "    for i, (audio, label) in enumerate(zip(*get_samples(subject_name))):\n",
        "\n",
        "        example = tf.train.Example(features=tf.train.Features(feature={\n",
        "                    'sample_id': _int_feauture(i),\n",
        "                    'subject_id': _int_feauture(subject_id),\n",
        "                    'label': _bytes_feauture(label.tobytes()),\n",
        "                    'raw_audio': _bytes_feauture(audio.tobytes()),\n",
        "                }))\n",
        "\n",
        "        writer.write(example.SerializeToString())\n",
        "        del audio, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJsOHDJbce6F",
        "colab_type": "code",
        "outputId": "da801f39-bd8d-4ea8-edcb-732461423234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "def main(directory):\n",
        "  for portion in portion_to_id.keys():\n",
        "    print(portion)\n",
        "\n",
        "    for subj_id in print_progress(portion_to_id[portion]):\n",
        "\n",
        "      writer = tf.python_io.TFRecordWriter(\n",
        "          (directory / 'tf_records' / portion / '{}.tfrecords'.format(subj_id)\n",
        "          ).as_posix())\n",
        "      serialize_sample(writer, subj_id)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main(Path('path_to_save_tfrecords'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train\n",
            "[====================] 100% (0/0) - done.                                       \n",
            "valid\n",
            "[====================] 100% (0/0) - done.                                       \n",
            "test\n",
            "[====================] 100% (0/0) - done.                                       \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jSy02i1dkeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5gex0P-eW6P",
        "colab_type": "text"
      },
      "source": [
        "Inception processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SW0NcU1Veg2O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\"\"\"Provides utilities to preprocess images for the Inception networks.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.python.ops import control_flow_ops\n",
        "\n",
        "\n",
        "def apply_with_random_selector(x, func, num_cases):\n",
        "  \"\"\"Computes func(x, sel), with sel sampled from [0...num_cases-1].\n",
        "  Args:\n",
        "    x: input Tensor.\n",
        "    func: Python function to apply.\n",
        "    num_cases: Python int32, number of cases to sample sel from.\n",
        "  Returns:\n",
        "    The result of func(x, sel), where func receives the value of the\n",
        "    selector as a python integer, but sel is sampled dynamically.\n",
        "  \"\"\"\n",
        "  sel = tf.random_uniform([], maxval=num_cases, dtype=tf.int32)\n",
        "  # Pass the real x only to one of the func calls.\n",
        "  return control_flow_ops.merge([\n",
        "      func(control_flow_ops.switch(x, tf.equal(sel, case))[1], case)\n",
        "      for case in range(num_cases)])[0]\n",
        "\n",
        "\n",
        "def distort_color(image, color_ordering=0, fast_mode=True, scope=None):\n",
        "  \"\"\"Distort the color of a Tensor image.\n",
        "  Each color distortion is non-commutative and thus ordering of the color ops\n",
        "  matters. Ideally we would randomly permute the ordering of the color ops.\n",
        "  Rather then adding that level of complication, we select a distinct ordering\n",
        "  of color ops for each preprocessing thread.\n",
        "  Args:\n",
        "    image: 3-D Tensor containing single image in [0, 1].\n",
        "    color_ordering: Python int, a type of distortion (valid values: 0-3).\n",
        "    fast_mode: Avoids slower ops (random_hue and random_contrast)\n",
        "    scope: Optional scope for name_scope.\n",
        "  Returns:\n",
        "    3-D Tensor color-distorted image on range [0, 1]\n",
        "  Raises:\n",
        "    ValueError: if color_ordering not in [0, 3]\n",
        "  \"\"\"\n",
        "  with tf.name_scope(scope, 'distort_color', [image]):\n",
        "    if fast_mode:\n",
        "      if color_ordering == 0:\n",
        "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
        "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "      else:\n",
        "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
        "    else:\n",
        "      if color_ordering == 0:\n",
        "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
        "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "        image = tf.image.random_hue(image, max_delta=0.2)\n",
        "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
        "      elif color_ordering == 1:\n",
        "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
        "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
        "        image = tf.image.random_hue(image, max_delta=0.2)\n",
        "      elif color_ordering == 2:\n",
        "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
        "        image = tf.image.random_hue(image, max_delta=0.2)\n",
        "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
        "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "      elif color_ordering == 3:\n",
        "        image = tf.image.random_hue(image, max_delta=0.2)\n",
        "        image = tf.image.random_saturation(image, lower=0.5, upper=1.5)\n",
        "        image = tf.image.random_contrast(image, lower=0.5, upper=1.5)\n",
        "        image = tf.image.random_brightness(image, max_delta=32. / 255.)\n",
        "      else:\n",
        "        raise ValueError('color_ordering must be in [0, 3]')\n",
        "\n",
        "    # The random_* ops do not necessarily clamp.\n",
        "    return tf.clip_by_value(image, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def distorted_bounding_box_crop(image,\n",
        "                                bbox,\n",
        "                                min_object_covered=0.1,\n",
        "                                aspect_ratio_range=(0.75, 1.33),\n",
        "                                area_range=(0.05, 1.0),\n",
        "                                max_attempts=100,\n",
        "                                scope=None):\n",
        "  \"\"\"Generates cropped_image using a one of the bboxes randomly distorted.\n",
        "  See `tf.image.sample_distorted_bounding_box` for more documentation.\n",
        "  Args:\n",
        "    image: 3-D Tensor of image (it will be converted to floats in [0, 1]).\n",
        "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
        "      where each coordinate is [0, 1) and the coordinates are arranged\n",
        "      as [ymin, xmin, ymax, xmax]. If num_boxes is 0 then it would use the whole\n",
        "      image.\n",
        "    min_object_covered: An optional `float`. Defaults to `0.1`. The cropped\n",
        "      area of the image must contain at least this fraction of any bounding box\n",
        "      supplied.\n",
        "    aspect_ratio_range: An optional list of `floats`. The cropped area of the\n",
        "      image must have an aspect ratio = width / height within this range.\n",
        "    area_range: An optional list of `floats`. The cropped area of the image\n",
        "      must contain a fraction of the supplied image within in this range.\n",
        "    max_attempts: An optional `int`. Number of attempts at generating a cropped\n",
        "      region of the image of the specified constraints. After `max_attempts`\n",
        "      failures, return the entire image.\n",
        "    scope: Optional scope for name_scope.\n",
        "  Returns:\n",
        "    A tuple, a 3-D Tensor cropped_image and the distorted bbox\n",
        "  \"\"\"\n",
        "  with tf.name_scope(scope, 'distorted_bounding_box_crop', [image, bbox]):\n",
        "    # Each bounding box has shape [1, num_boxes, box coords] and\n",
        "    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n",
        "\n",
        "    # A large fraction of image datasets contain a human-annotated bounding\n",
        "    # box delineating the region of the image containing the object of interest.\n",
        "    # We choose to create a new bounding box for the object which is a randomly\n",
        "    # distorted version of the human-annotated bounding box that obeys an\n",
        "    # allowed range of aspect ratios, sizes and overlap with the human-annotated\n",
        "    # bounding box. If no box is supplied, then we assume the bounding box is\n",
        "    # the entire image.\n",
        "    sample_distorted_bounding_box = tf.image.sample_distorted_bounding_box(\n",
        "        tf.shape(image),\n",
        "        bounding_boxes=bbox,\n",
        "        min_object_covered=min_object_covered,\n",
        "        aspect_ratio_range=aspect_ratio_range,\n",
        "        area_range=area_range,\n",
        "        max_attempts=max_attempts,\n",
        "        use_image_if_no_bounding_boxes=True)\n",
        "    bbox_begin, bbox_size, distort_bbox = sample_distorted_bounding_box\n",
        "\n",
        "    # Crop the image to the specified bounding box.\n",
        "    cropped_image = tf.slice(image, bbox_begin, bbox_size)\n",
        "    return cropped_image, distort_bbox\n",
        "\n",
        "\n",
        "def preprocess_for_train(image, height, width, bbox,\n",
        "                         fast_mode=True,\n",
        "                         scope=None):\n",
        "  \"\"\"Distort one image for training a network.\n",
        "  Distorting images provides a useful technique for augmenting the data\n",
        "  set during training in order to make the network invariant to aspects\n",
        "  of the image that do not effect the label.\n",
        "  Additionally it would create image_summaries to display the different\n",
        "  transformations applied to the image.\n",
        "  Args:\n",
        "    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n",
        "      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n",
        "      is [0, MAX], where MAX is largest positive representable number for\n",
        "      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details).\n",
        "    height: integer\n",
        "    width: integer\n",
        "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
        "      where each coordinate is [0, 1) and the coordinates are arranged\n",
        "      as [ymin, xmin, ymax, xmax].\n",
        "    fast_mode: Optional boolean, if True avoids slower transformations (i.e.\n",
        "      bi-cubic resizing, random_hue or random_contrast).\n",
        "    scope: Optional scope for name_scope.\n",
        "  Returns:\n",
        "    3-D float Tensor of distorted image used for training with range [-1, 1].\n",
        "  \"\"\"\n",
        "  with tf.name_scope(scope, 'distort_image', [image, height, width, bbox]):\n",
        "    if bbox is None:\n",
        "      bbox = tf.constant([0.0, 0.0, 1.0, 1.0],\n",
        "                         dtype=tf.float32,\n",
        "                         shape=[1, 1, 4])\n",
        "    if image.dtype != tf.float32:\n",
        "      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    # Each bounding box has shape [1, num_boxes, box coords] and\n",
        "    # the coordinates are ordered [ymin, xmin, ymax, xmax].\n",
        "    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),\n",
        "                                                  bbox)\n",
        "    tf.image_summary('image_with_bounding_boxes', image_with_box)\n",
        "\n",
        "    distorted_image, distorted_bbox = distorted_bounding_box_crop(image, bbox)\n",
        "    # Restore the shape since the dynamic slice based upon the bbox_size loses\n",
        "    # the third dimension.\n",
        "    distorted_image.set_shape([None, None, 3])\n",
        "    image_with_distorted_box = tf.image.draw_bounding_boxes(\n",
        "        tf.expand_dims(image, 0), distorted_bbox)\n",
        "    tf.image_summary('images_with_distorted_bounding_box',\n",
        "                     image_with_distorted_box)\n",
        "\n",
        "    # This resizing operation may distort the images because the aspect\n",
        "    # ratio is not respected. We select a resize method in a round robin\n",
        "    # fashion based on the thread number.\n",
        "    # Note that ResizeMethod contains 4 enumerated resizing methods.\n",
        "\n",
        "    # We select only 1 case for fast_mode bilinear.\n",
        "    num_resize_cases = 1 if fast_mode else 4\n",
        "    distorted_image = apply_with_random_selector(\n",
        "        distorted_image,\n",
        "        lambda x, method: tf.image.resize_images(x, [height, width], method=method),\n",
        "        num_cases=num_resize_cases)\n",
        "\n",
        "    tf.image_summary('cropped_resized_image',\n",
        "                     tf.expand_dims(distorted_image, 0))\n",
        "\n",
        "    # Randomly flip the image horizontally.\n",
        "    distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
        "\n",
        "    # Randomly distort the colors. There are 4 ways to do it.\n",
        "    distorted_image = apply_with_random_selector(\n",
        "        distorted_image,\n",
        "        lambda x, ordering: distort_color(x, ordering, fast_mode),\n",
        "        num_cases=4)\n",
        "\n",
        "    tf.image_summary('final_distorted_image',\n",
        "                     tf.expand_dims(distorted_image, 0))\n",
        "    distorted_image = tf.sub(distorted_image, 0.5)\n",
        "    distorted_image = tf.mul(distorted_image, 2.0)\n",
        "    return distorted_image\n",
        "\n",
        "\n",
        "def preprocess_for_eval(image, height, width,\n",
        "                        central_fraction=0.875, scope=None):\n",
        "  \"\"\"Prepare one image for evaluation.\n",
        "  If height and width are specified it would output an image with that size by\n",
        "  applying resize_bilinear.\n",
        "  If central_fraction is specified it would cropt the central fraction of the\n",
        "  input image.\n",
        "  Args:\n",
        "    image: 3-D Tensor of image. If dtype is tf.float32 then the range should be\n",
        "      [0, 1], otherwise it would converted to tf.float32 assuming that the range\n",
        "      is [0, MAX], where MAX is largest positive representable number for\n",
        "      int(8/16/32) data type (see `tf.image.convert_image_dtype` for details)\n",
        "    height: integer\n",
        "    width: integer\n",
        "    central_fraction: Optional Float, fraction of the image to crop.\n",
        "    scope: Optional scope for name_scope.\n",
        "  Returns:\n",
        "    3-D float Tensor of prepared image.\n",
        "  \"\"\"\n",
        "  with tf.name_scope(scope, 'eval_image', [image, height, width]):\n",
        "    if image.dtype != tf.float32:\n",
        "      image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    # Crop the central region of the image with an area containing 87.5% of\n",
        "    # the original image.\n",
        "    if central_fraction:\n",
        "      image = tf.image.central_crop(image, central_fraction=central_fraction)\n",
        "\n",
        "    if height and width:\n",
        "      # Resize the image to the specified height and width.\n",
        "      image = tf.expand_dims(image, 0)\n",
        "      image = tf.image.resize_bilinear(image, [height, width],\n",
        "                                       align_corners=False)\n",
        "      image = tf.squeeze(image, [0])\n",
        "    image = tf.sub(image, 0.5)\n",
        "    image = tf.mul(image, 2.0)\n",
        "    return image\n",
        "\n",
        "\n",
        "def preprocess_image(image, height, width,\n",
        "                     is_training=False,\n",
        "                     bbox=None,\n",
        "                     fast_mode=True):\n",
        "  \"\"\"Pre-process one image for training or evaluation.\n",
        "  Args:\n",
        "    image: 3-D Tensor [height, width, channels] with the image.\n",
        "    height: integer, image expected height.\n",
        "    width: integer, image expected width.\n",
        "    is_training: Boolean. If true it would transform an image for train,\n",
        "      otherwise it would transform it for evaluation.\n",
        "    bbox: 3-D float Tensor of bounding boxes arranged [1, num_boxes, coords]\n",
        "      where each coordinate is [0, 1) and the coordinates are arranged as\n",
        "      [ymin, xmin, ymax, xmax].\n",
        "    fast_mode: Optional boolean, if True avoids slower transformations.\n",
        "  Returns:\n",
        "    3-D float Tensor containing an appropriately scaled image\n",
        "  Raises:\n",
        "    ValueError: if user does not provide bounding box\n",
        "  \"\"\"\n",
        "  if is_training:\n",
        "    return preprocess_for_train(image, height, width, bbox, fast_mode)\n",
        "  else:\n",
        "    return preprocess_for_eval(image, height, width)\n",
        "# after inception data provider\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrIr00-Jd-RW",
        "colab_type": "code",
        "outputId": "f88347f5-a209-478c-85d8-36ceb27ad80b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "from pathlib import Path\n",
        "# line deleted\n",
        "\n",
        "slim = tf.contrib.slim\n",
        "\n",
        "def get_split(dataset_dir, is_training=True, split_name='train', batch_size=32,\n",
        "              seq_length=100, debugging=False):\n",
        "    \"\"\"Returns a data split of the RECOLA dataset, which was saved in tfrecords format.\n",
        "    Args:\n",
        "        split_name: A train/test/valid split name.\n",
        "    Returns:\n",
        "        The raw audio examples and the corresponding arousal/valence\n",
        "        labels.\n",
        "    \"\"\"\n",
        "\n",
        "    root_path = Path(dataset_dir) / split_name\n",
        "    paths = [str(x) for x in root_path.glob('*.tfrecords')]\n",
        "\n",
        "    filename_queue = tf.train.string_input_producer(paths, shuffle=is_training)\n",
        "\n",
        "    reader = tf.TFRecordReader()\n",
        "\n",
        "    _, serialized_example = reader.read(filename_queue)\n",
        "\n",
        "    features = tf.parse_single_example(\n",
        "        serialized_example,\n",
        "        features={\n",
        "            'raw_audio': tf.FixedLenFeature([], tf.string),\n",
        "            'label': tf.FixedLenFeature([], tf.string),\n",
        "            'subject_id': tf.FixedLenFeature([], tf.int64),\n",
        "            'frame': tf.FixedLenFeature([], tf.string),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    raw_audio = tf.decode_raw(features['raw_audio'], tf.float32)\n",
        "    frame = tf.image.decode_jpeg(features['frame'])\n",
        "    label = tf.decode_raw(features['label'], tf.float32)\n",
        "    subject_id = features['subject_id']\n",
        "\n",
        "    # 640 samples at 16KhZ corresponds to 40ms which is the frequency of\n",
        "    # annotations.\n",
        "    raw_audio.set_shape([640])\n",
        "    label.set_shape([2])\n",
        "    frame.set_shape([96, 96, 3])\n",
        "    frame = tf.cast(frame, tf.float32) / 255.\n",
        "\n",
        "    if is_training:\n",
        "        resized_image = tf.image.resize_images(frame, [110, 110])\n",
        "        frame = tf.random_crop(resized_image, [96, 96, 3])\n",
        "        frame = distort_color(frame, 1)\n",
        "\n",
        "    # Number of threads should always be one, in order to load samples\n",
        "    # sequentially.\n",
        "    frames, audio_samples, labels, subject_ids = tf.train.batch(\n",
        "        [frame, raw_audio, label, subject_id], seq_length, num_threads=1, capacity=1000)\n",
        "\n",
        "\n",
        "    # Assert is an expensive op so we only want to use it when it's a must.\n",
        "    if debugging:\n",
        "        # Asserts that a sequence contains samples from a single subject.\n",
        "        assert_op = tf.Assert(\n",
        "            tf.reduce_all(tf.equal(subject_ids[0], subject_ids)),\n",
        "            [subject_ids])\n",
        "\n",
        "        with tf.control_dependencies([assert_op]):\n",
        "            audio_samples = tf.identity(audio_samples)\n",
        "\n",
        "    audio_samples = tf.expand_dims(audio_samples, 0)\n",
        "    labels = tf.expand_dims(labels, 0)\n",
        "    frames = tf.expand_dims(frames, 0)\n",
        "\n",
        "    if is_training:\n",
        "        frames, audio_samples, labels, subject_ids = tf.train.shuffle_batch(\n",
        "            [frames, audio_samples, labels, subject_ids], batch_size, 1000, 50, num_threads=1)\n",
        "    else:\n",
        "        frames, audio_samples, labels, subject_ids = tf.train.batch(\n",
        "            [frames, audio_samples, labels, subject_ids], batch_size, num_threads=1, capacity=1000)\n",
        "\n",
        "    return frames[:, 0, :, :], audio_samples[:, 0, :, :], labels[:, 0, :, :], subject_ids"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0721 17:27:49.401975 139738847573888 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbDBEgVPlQxf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}